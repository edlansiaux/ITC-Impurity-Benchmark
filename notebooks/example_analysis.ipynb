{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des Résultats ITC\n",
    "\n",
    "Ce notebook présente une analyse interactive des résultats du benchmark ITC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils.visualization import ResultsVisualizer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des résultats\n",
    "results = pd.read_csv('../results/raw_results/individual_metrics_results.csv')\n",
    "print(f\"Dimensions des données: {results.shape}\")\n",
    "print(f\"Métriques évaluées: {results['metric'].nunique()}\")\n",
    "print(f\"Datasets utilisés: {results['dataset'].nunique()}\")\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance moyenne par métrique\n",
    "performance = results.groupby('metric').agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'tree_depth': ['mean', 'std'],\n",
    "    'training_time': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "performance.columns = ['_'.join(col).strip() for col in performance.columns.values]\n",
    "performance = performance.sort_values('accuracy_mean', ascending=False)\n",
    "\n",
    "print(\"Top 10 des métriques par accuracy:\")\n",
    "performance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des performances\n",
    "visualizer = ResultsVisualizer()\n",
    "fig = visualizer.create_performance_comparison_plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse détaillée d'ITC vs Gini\n",
    "itc_vs_gini = results[results['metric'].isin(['itc', 'gini'])]\n",
    "\n",
    "comparison = itc_vs_gini.groupby(['dataset', 'metric'])['accuracy'].mean().unstack()\n",
    "comparison['improvement'] = ((comparison['itc'] - comparison['gini']) / comparison['gini']) * 100\n",
    "\n",
    "print(\"Amélioration d'ITC vs Gini par dataset:\")\n",
    "print(comparison.sort_values('improvement', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de Sensibilité des Paramètres ITC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des résultats de sensibilité\n",
    "try:\n",
    "    sensitivity_results = pd.read_csv('../results/raw_results/itc_parameter_sensitivity.csv')\n",
    "    \n",
    "    # Heatmap de performance par paramètres\n",
    "    pivot_data = sensitivity_results.groupby(['alpha', 'gamma'])['accuracy'].mean().unstack()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='viridis')\n",
    "    plt.title('Accuracy moyenne par paramètres α et γ (β=3.5)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Fichier de sensibilité non trouvé. Exécutez d'abord l'analyse de sensibilité.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
